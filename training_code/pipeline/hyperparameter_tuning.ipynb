{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 08:33:57.996568: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Sequence\n",
    "\n",
    "import joblib\n",
    "# add directory up to path to get main naming script\n",
    "from optuna.pruners import MedianPruner\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, Any, Callable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "\n",
    "from dataclasses import dataclass, field, asdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NCP_SEED = 22222\n",
    "IMAGE_SHAPE = (144, 256, 3)\n",
    "IMAGE_SHAPE_CV = (IMAGE_SHAPE[1], IMAGE_SHAPE[0])\n",
    "\n",
    "# helper classes that contain all the parameters in the generate_*_model functions\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "    # dataclasses can't have non-default follow default\n",
    "    seq_len: int = field(default=False, init=True)\n",
    "    image_shape: Tuple[int, int, int] = IMAGE_SHAPE\n",
    "    augmentation_params: Optional[Dict] = None\n",
    "    batch_size: Optional[int] = None\n",
    "    single_step: bool = False\n",
    "    no_norm_layer: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NCPParams(ModelParams):\n",
    "    seed: int = DEFAULT_NCP_SEED\n",
    "\n",
    "COMMON_TRAIN_PARAMS = {\n",
    "    \"epochs\": 100,\n",
    "    \"val_split\": 0.05,\n",
    "    \"opt\": \"adam\",\n",
    "    \"data_shift\": 16,\n",
    "    \"data_stride\": 1,\n",
    "    \"cached_data_dir\": \"cached_data\",\n",
    "    \"save_period\": 20,\n",
    "}\n",
    "COMMON_MODEL_PARAMS = {\n",
    "    \"seq_len\": 64,\n",
    "    \"single_step\": False,\n",
    "    \"no_norm_layer\": False,\n",
    "    \"augmentation_params\": {\n",
    "        \"noise\": 0.05,\n",
    "        \"sequence_params\": {\n",
    "            \"brightness\": 0.4,\n",
    "            \"contrast\": 0.4,\n",
    "            \"saturation\": 0.4,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_val_train_loss(logs):\n",
    "    return logs[\"loss\"] + logs[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasPruningCallbackFunction(TFKerasPruningCallback):\n",
    "    \"\"\"\n",
    "    Convenience class that allows pruning based on any function of the logs, instead of just looking at 1\n",
    "    log metric\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trial: optuna.trial.Trial, get_objective: Callable) -> None:\n",
    "        super().__init__(trial, \"\")\n",
    "        self.get_objective = get_objective\n",
    "\n",
    "    # copied from optuna/integration/keras.py\n",
    "    def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, float]] = None) -> None:\n",
    "        logs = logs or {}\n",
    "        current_score = self.get_objective(logs)\n",
    "        if current_score is None:\n",
    "            message = (\n",
    "                \"The metric '{}' is not in the evaluation logs for pruning. \"\n",
    "                \"Please make sure you set the correct metric name.\".format(self._monitor)\n",
    "            )\n",
    "            warnings.warn(message)\n",
    "            return\n",
    "        # logging a nan obj leads to crash\n",
    "        if np.isnan(current_score):\n",
    "            message = f\"Trial was pruned at epoch {epoch} because objective value was NaN\"\n",
    "            raise optuna.TrialPruned(message)\n",
    "\n",
    "        self._trial.report(float(current_score), step=epoch)\n",
    "        if self._trial.should_prune():\n",
    "            message = \"Trial was pruned at epoch {}.\".format(epoch)\n",
    "            raise optuna.TrialPruned(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objective(trial: Trial, result: Tuple[History, str]):\n",
    "    \"\"\"\n",
    "    Calculates objective value from history of losses and also logs train_loss and val_loss separately\n",
    "\n",
    "    @param trial: optuna trial\n",
    "    @param result: Tensorflow history object returned by trainer\n",
    "    @return: objective value\n",
    "    \"\"\"\n",
    "    history, time_str = result\n",
    "    trial.set_user_attr(\"checkpoint_time_str\", time_str)\n",
    "\n",
    "    losses = np.array([[epoch_train_loss, epoch_val_loss] for epoch_train_loss, epoch_val_loss in\n",
    "                       zip(history.history[\"loss\"], history.history[\"val_loss\"])])\n",
    "    loss_sums = losses.sum(axis=1)\n",
    "    best_epoch = np.argmin(loss_sums)\n",
    "    trial.set_user_attr(\"sum_train_loss\", losses[best_epoch, 0])\n",
    "    trial.set_user_attr(\"sum_val_loss\", losses[best_epoch, 1])\n",
    "    trial.set_user_attr(\"best_sum_epoch\", int(best_epoch))\n",
    "\n",
    "    # calculate best train and val epochs\n",
    "    best_train = np.argmin(losses[:, 0])\n",
    "    trial.set_user_attr(\"best_train_epoch\", int(best_train))\n",
    "    trial.set_user_attr(\"best_train_loss\", losses[best_train, 0])\n",
    "    best_val = np.argmin(losses[:, 1])\n",
    "    trial.set_user_attr(\"best_val_epoch\", int(best_val))\n",
    "    trial.set_user_attr(\"best_val_loss\", losses[best_val, 1])\n",
    "\n",
    "    trial.set_user_attr(\"trial_time\", time.time())\n",
    "\n",
    "    objective = loss_sums[best_epoch]\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna objetive functions\n",
    "def ncp_objective(trial: Trial, data_dir: str, batch_size: int, **train_kwargs: Dict[str, Any]):\n",
    "    # get trial params from bayesian optimization\n",
    "    seeds_to_try = list(range(22221, 22230)) + [55555]\n",
    "    ncp_seed = trial.suggest_categorical(\"ncp_seed\", seeds_to_try)\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", low=1e-5, high=1e-2, log=True)\n",
    "    decay_rate = trial.suggest_float(\"decay_rate\", 0.85, 1)\n",
    "\n",
    "    prune_callback = [KerasPruningCallbackFunction(trial, sum_val_train_loss)]\n",
    "\n",
    "    model_params = NCPParams(seed=ncp_seed, **COMMON_MODEL_PARAMS)\n",
    "    merged_kwargs = copy.deepcopy(COMMON_TRAIN_PARAMS)\n",
    "    merged_kwargs.update(**train_kwargs)\n",
    "    history = train_model(lr=lr, decay_rate=decay_rate, callbacks=prune_callback,\n",
    "                          model_params=model_params, data_dir=data_dir, batch_size=batch_size, **merged_kwargs)\n",
    "    trial.set_user_attr(\"model_params\", repr(model_params))\n",
    "\n",
    "    return calculate_objective(trial, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(obj_fn: Callable, data_dir: str, study_name: str, n_trials: int, batch_size: int,\n",
    "                             timeout: float = None, storage_name: str = \"sqlite:///hyperparam_tuning.db\",\n",
    "                             save_pkl: bool = False, train_kwargs: Optional[Dict[str, Any]] = None):\n",
    "    optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "    study_name_network = f\"{study_name}{obj_fn.__name__}\"\n",
    "    study_params = {\n",
    "        \"study_name\": study_name_network,\n",
    "        \"load_if_exists\": True,\n",
    "        \"direction\": \"minimize\",\n",
    "        \"pruner\": MedianPruner(n_warmup_steps=10, n_min_trials=3),\n",
    "    }\n",
    "\n",
    "    if save_pkl:\n",
    "        path_relative = os.path.join(SCRIPT_DIR, storage_name)\n",
    "        if os.path.exists(path_relative):\n",
    "            study = joblib.load(path_relative)\n",
    "        else:\n",
    "            print(f\"No existing study found at path {path_relative}. Creating a new one\")\n",
    "            study = optuna.create_study(**study_params)\n",
    "    else:\n",
    "        study = optuna.create_study(storage=storage_name, **study_params)\n",
    "\n",
    "    if train_kwargs is None:\n",
    "        train_kwargs = {}\n",
    "\n",
    "    # only continue training up to n_trials trials total\n",
    "    current_num_trials = len(study.trials)\n",
    "    remaining_trials = n_trials-current_num_trials\n",
    "\n",
    "    study.optimize(functools.partial(obj_fn, data_dir=data_dir, batch_size=batch_size, **train_kwargs),\n",
    "                   n_trials=remaining_trials, timeout=timeout)\n",
    "\n",
    "    if save_pkl:\n",
    "        joblib.dump(study, storage_name)\n",
    "    return study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
