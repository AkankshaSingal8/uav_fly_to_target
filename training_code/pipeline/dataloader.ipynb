{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import kerasncp as kncp\n",
    "\n",
    "import os\n",
    "from typing import Iterable, Dict\n",
    "import tensorflow as tf\n",
    "import kerasncp as kncp\n",
    "from kerasncp.tf import LTCCell, WiredCfcCell\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from matplotlib.image import imread\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras_models import generate_ncp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_output_normalization(root):\n",
    "    training_output_mean_fn = os.path.join(root, 'stats', 'training_output_means.csv')\n",
    "    if os.path.exists(training_output_mean_fn):\n",
    "        print('Loading training data output means from: %s' % training_output_mean_fn)\n",
    "        output_means = np.genfromtxt(training_output_mean_fn, delimiter=',')\n",
    "    else:\n",
    "        output_means = np.zeros(4)\n",
    "\n",
    "    training_output_std_fn = os.path.join(root, 'stats', 'training_output_stds.csv')\n",
    "    if os.path.exists(training_output_std_fn):\n",
    "        print('Loading training data output std from: %s' % training_output_std_fn)\n",
    "        output_stds = np.genfromtxt(training_output_std_fn, delimiter=',')\n",
    "    else:\n",
    "        output_stds = np.ones(4)\n",
    "\n",
    "    return output_means, output_stds\n",
    "\n",
    "\n",
    "def load_dataset_multi(root, image_size, seq_len, shift, stride, label_scale):\n",
    "    file_ending = 'png'\n",
    "    IMAGE_SHAPE = (144, 256, 3)\n",
    "    IMAGE_SHAPE_CV = (IMAGE_SHAPE[1], IMAGE_SHAPE[0])\n",
    "\n",
    "    def sub_to_batch(sub_feature, sub_label):\n",
    "        sfb = sub_feature.batch(seq_len, drop_remainder=True)\n",
    "        slb = sub_label.batch(seq_len, drop_remainder=True)\n",
    "        return tf.data.Dataset.zip((sfb, slb))\n",
    "        # return sub.batch(seq_len, drop_remainder=True)\n",
    "    \n",
    "    def apply_random_augmentations(image):\n",
    "        # Generate a random number and apply augmentations with a 50% probability\n",
    "        if tf.random.uniform(()) > 0.3:  # 30% chance to apply augmentations\n",
    "            image = tf.image.convert_image_dtype(image, tf.float32)  # Convert to float32 for augmentation\n",
    "            image = tf.image.random_brightness(image, max_delta=0.1)  # Random brightness adjustment\n",
    "            image = tf.image.random_contrast(image, lower=0.8, upper=1.2)  # Random contrast adjustment\n",
    "            image = tf.image.random_saturation(image, lower=0.8, upper=1.2)  # Random saturation adjustment\n",
    "            image = tf.image.convert_image_dtype(image, tf.uint8)  # Convert back to uint8\n",
    "        return image\n",
    "\n",
    "    \n",
    "    datasets = []\n",
    "\n",
    "    #output_means, output_stds = get_output_normalization(root)\n",
    "\n",
    "    \n",
    "    for i in range(len(os.listdir(root))):\n",
    "        directory = i + 1\n",
    "        csv_file_name = f\"{root}/{str(directory)}/data_out.csv\"\n",
    "        labels = np.genfromtxt(csv_file_name, delimiter=',', skip_header=1, dtype=np.float32)\n",
    "        print(\"labels\", labels)\n",
    "        # if labels.shape[1] == 4:\n",
    "        #     labels = (labels - output_means) / output_stds\n",
    "        #     # labels = labels * label_scale\n",
    "        # elif labels.shape[1] == 5:\n",
    "        #     labels = (labels[:, 1:] - output_means) / output_stds\n",
    "        #     # labels = labels[:,1:] * label_scale\n",
    "        # else:\n",
    "        #     raise Exception('Wrong size of input data (expected 4, got %d' % labels.shape[1])\n",
    "    \n",
    "        labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "        # n_images = len(os.listdir(os.path.join(root, d))) - 1\n",
    "        n_images = len([fn for fn in os.listdir(f\"./{root}/{str(directory)}\") if file_ending in fn])\n",
    "        print(n_images)\n",
    "        print(\"no of imgs\", n_images)\n",
    "        # dataset_np = np.empty((n_images, 256, 256, 3), dtype=np.uint8)\n",
    "        dataset_np = np.empty((n_images, *image_size), dtype=np.uint8)\n",
    "\n",
    "        for ix in range(n_images):\n",
    "            # dataset_np[ix] = imread(os.path.join(root, d, '%06d.jpeg' % ix))\n",
    "            img_file_name = root + \"/\" + str(directory) +'/Image' + str(ix + 1) + '.'+ file_ending\n",
    "            img = Image.open(img_file_name)\n",
    "            img = img.resize(IMAGE_SHAPE_CV)\n",
    "            # dataset_np[ix] = img[img.height - image_size[0]:, :, :]\n",
    "            dataset_np[ix] = img\n",
    "\n",
    "        images_dataset = tf.data.Dataset.from_tensor_slices(dataset_np)\n",
    "        images_dataset = images_dataset.map(apply_random_augmentations, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = tf.data.Dataset.zip((images_dataset, labels_dataset))\n",
    "        dataset = dataset.window(seq_len, shift=shift, stride=stride, drop_remainder=True).flat_map(sub_to_batch)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def get_dataset_multi(root, image_size, seq_len, shift, stride, validation_ratio, label_scale, extra_data_root=None):\n",
    "    ds = load_dataset_multi(root, image_size, seq_len, shift, stride, label_scale)\n",
    "    print('n bags: %d' % len(ds))\n",
    "    cnt = 0\n",
    "\n",
    "    for d in ds:\n",
    "        for (ix, _) in enumerate(d):\n",
    "            pass\n",
    "            cnt += ix\n",
    "    print('n windows: %d' % cnt)\n",
    "\n",
    "    val_ix = 0\n",
    "\n",
    "    # val_ix = int(len(ds) * validation_ratio)\n",
    "    # print('\\nval_ix: %d\\n' % val_ix)\n",
    "    # validation_datasets = ds[:val_ix]\n",
    "\n",
    "    training_datasets = ds[val_ix:]\n",
    "\n",
    "    # if either dataset has length 0, trying to call flat map raises error that return type is wrong\n",
    "    # assert len(training_datasets) > 0 and len(validation_datasets) > 0, f\"Training or validation dataset has no points!\" \\\n",
    "    #                                                                     f\"Train dataset len: {len(training_datasets)}\" \\\n",
    "    #                                                                     f\"Val dataset len: {len(validation_datasets)}\"\n",
    "    training = tf.data.Dataset.from_tensor_slices(training_datasets).flat_map(lambda x: x)\n",
    "    # validation = tf.data.Dataset.from_tensor_slices(validation_datasets).flat_map(lambda x: x)\n",
    "\n",
    "    # return training, validation\n",
    "    return training\n",
    "\n",
    "def load_val_dataset_multi(root, image_size, seq_len, shift, stride, label_scale):\n",
    "    file_ending = 'png'\n",
    "    IMAGE_SHAPE = (144, 256, 3)\n",
    "    IMAGE_SHAPE_CV = (IMAGE_SHAPE[1], IMAGE_SHAPE[0])\n",
    "\n",
    "    def sub_to_batch(sub_feature, sub_label):\n",
    "        sfb = sub_feature.batch(seq_len, drop_remainder=True)\n",
    "        slb = sub_label.batch(seq_len, drop_remainder=True)\n",
    "        return tf.data.Dataset.zip((sfb, slb))\n",
    "        # return sub.batch(seq_len, drop_remainder=True)\n",
    "\n",
    "    \n",
    "    datasets = []\n",
    "\n",
    "    #output_means, output_stds = get_output_normalization(root)\n",
    "\n",
    "    \n",
    "    for i in range(len(os.listdir(root))):\n",
    "        directory = i + 1\n",
    "        csv_file_name = f\"{root}/{str(directory)}/data_out.csv\"\n",
    "        labels = np.genfromtxt(csv_file_name, delimiter=',', skip_header=1, dtype=np.float32)\n",
    "        print(\"labels\", labels)\n",
    "        # if labels.shape[1] == 4:\n",
    "        #     labels = (labels - output_means) / output_stds\n",
    "        #     # labels = labels * label_scale\n",
    "        # elif labels.shape[1] == 5:\n",
    "        #     labels = (labels[:, 1:] - output_means) / output_stds\n",
    "        #     # labels = labels[:,1:] * label_scale\n",
    "        # else:\n",
    "        #     raise Exception('Wrong size of input data (expected 4, got %d' % labels.shape[1])\n",
    "    \n",
    "        labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "        # n_images = len(os.listdir(os.path.join(root, d))) - 1\n",
    "        base_name = f\"{root}/{directory}\"\n",
    "        n_images = len([fn for fn in os.listdir(base_name) if file_ending in fn])\n",
    "        print(n_images)\n",
    "        print(\"no of imgs\", n_images)\n",
    "        # dataset_np = np.empty((n_images, 256, 256, 3), dtype=np.uint8)\n",
    "        dataset_np = np.empty((n_images, *image_size), dtype=np.uint8)\n",
    "\n",
    "        for ix in range(n_images):\n",
    "            # dataset_np[ix] = imread(os.path.join(root, d, '%06d.jpeg' % ix))\n",
    "            img_file_name = root + \"/\" + str(directory) +'/Image' + str(ix + 1) + '.'+ file_ending\n",
    "            img = Image.open(img_file_name)\n",
    "            img = img.resize(IMAGE_SHAPE_CV)\n",
    "            # dataset_np[ix] = img[img.height - image_size[0]:, :, :]\n",
    "            dataset_np[ix] = img\n",
    "\n",
    "        images_dataset = tf.data.Dataset.from_tensor_slices(dataset_np)\n",
    "        dataset = tf.data.Dataset.zip((images_dataset, labels_dataset))\n",
    "        dataset = dataset.window(seq_len, shift=shift, stride=stride, drop_remainder=True).flat_map(sub_to_batch)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def get_val_dataset_multi(root, image_size, seq_len, shift, stride, validation_ratio, label_scale, extra_data_root=None):\n",
    "    ds = load_val_dataset_multi(root, image_size, seq_len, shift, stride, label_scale)\n",
    "    print('n bags: %d' % len(ds))\n",
    "    cnt = 0\n",
    "\n",
    "    for d in ds:\n",
    "        for (ix, _) in enumerate(d):\n",
    "            pass\n",
    "            cnt += ix\n",
    "    print('n windows: %d' % cnt)\n",
    "\n",
    "    val_ix = 0\n",
    "\n",
    "    # val_ix = int(len(ds) * validation_ratio)\n",
    "    # print('\\nval_ix: %d\\n' % val_ix)\n",
    "    # validation_datasets = ds[:val_ix]\n",
    "\n",
    "    training_datasets = ds[val_ix:]\n",
    "\n",
    "    # if either dataset has length 0, trying to call flat map raises error that return type is wrong\n",
    "    # assert len(training_datasets) > 0 and len(validation_datasets) > 0, f\"Training or validation dataset has no points!\" \\\n",
    "    #                                                                     f\"Train dataset len: {len(training_datasets)}\" \\\n",
    "    #                                                                     f\"Val dataset len: {len(validation_datasets)}\"\n",
    "    training = tf.data.Dataset.from_tensor_slices(training_datasets).flat_map(lambda x: x)\n",
    "    # validation = tf.data.Dataset.from_tensor_slices(validation_datasets).flat_map(lambda x: x)\n",
    "\n",
    "    # return training, validation\n",
    "    return training"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
